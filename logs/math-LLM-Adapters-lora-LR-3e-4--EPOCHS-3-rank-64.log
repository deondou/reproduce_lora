[2024-10-07 11:35:33,787] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-10-07 11:35:34,765] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-07 11:35:34,765] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/milora/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed configs/stage2.conf --model_name_or_path /root/autodl-tmp/meta-llama/Llama-2-7b-hf --output_dir /math/LLM-Adapters-lora-LR-3e-4--EPOCHS-3-rank-64 --lora_r 64 --lora_alpha 128 --lora_dropout 0.05 --target_modules q_proj,k_proj,v_proj,up_proj,down_proj --data_path meta-math/MetaMathQA --dataset_split train[:100000] --dataset_field query response --model_max_length 2048 --num_train_epochs 3 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --save_strategy epoch --learning_rate 3e-4 --weight_decay 0. --warmup_ratio 0 --warmup_steps 100 --lr_scheduler_type linear --logging_steps 1 --bf16 True --tf32 True --method_type lora --report_to tensorboard
[2024-10-07 11:35:36,780] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.17.1-1
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-10-07 11:35:37,804] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
[2024-10-07 11:35:37,804] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2024-10-07 11:35:37,804] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-10-07 11:35:37,804] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-10-07 11:35:37,804] [INFO] [launch.py:164:main] dist_world_size=1
[2024-10-07 11:35:37,804] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-10-07 11:35:37,805] [INFO] [launch.py:256:main] process 5064 spawned with command: ['/root/miniconda3/envs/milora/bin/python', '-u', 'train.py', '--local_rank=0', '--deepspeed', 'configs/stage2.conf', '--model_name_or_path', '/root/autodl-tmp/meta-llama/Llama-2-7b-hf', '--output_dir', '/math/LLM-Adapters-lora-LR-3e-4--EPOCHS-3-rank-64', '--lora_r', '64', '--lora_alpha', '128', '--lora_dropout', '0.05', '--target_modules', 'q_proj,k_proj,v_proj,up_proj,down_proj', '--data_path', 'meta-math/MetaMathQA', '--dataset_split', 'train[:100000]', '--dataset_field', 'query', 'response', '--model_max_length', '2048', '--num_train_epochs', '3', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--save_strategy', 'epoch', '--learning_rate', '3e-4', '--weight_decay', '0.', '--warmup_ratio', '0', '--warmup_steps', '100', '--lr_scheduler_type', 'linear', '--logging_steps', '1', '--bf16', 'True', '--tf32', 'True', '--method_type', 'lora', '--report_to', 'tensorboard']
train YEEAHH train.py
[2024-10-07 11:35:40,496] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-10-07 11:35:41,347] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-10-07 11:35:41,347] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
script_args.target_modules ['q_proj,k_proj,v_proj,up_proj,down_proj']
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
adapter_name_or_path=None,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_path=meta-math/MetaMathQA,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_field=['query', 'response'],
dataset_split=train[:100000],
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/stage2.conf,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
init_lora_weights=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/math/LLM-Adapters-lora-LR-3e-4--EPOCHS-3-rank-64/runs/Oct07_11-35-40_autodl-container-6e334ba6d7-3710c85a,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lora_alpha=128,
lora_dropout=0.05,
lora_r=64,
loraplus_lr_ratio=4,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
method_type=lora,
metric_for_best_model=None,
model_max_length=2048,
model_name_or_path=/root/autodl-tmp/meta-llama/Llama-2-7b-hf,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/math/LLM-Adapters-lora-LR-3e-4--EPOCHS-3-rank-64,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/math/LLM-Adapters-lora-LR-3e-4--EPOCHS-3-rank-64,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
target_modules=['q_proj,k_proj,v_proj,up_proj,down_proj'],
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_adalora=False,
use_cpu=False,
use_dora=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
use_rslora=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.0,
)
Loading checkpoint shards:   0%|                                                                                                                | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|████████████████████████████████████████████████████                                                    | 1/2 [00:01<00:01,  1.74s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.19s/it]
script_args.model_name_or_path /root/autodl-tmp/meta-llama/Llama-2-7b-hf
script_args.adapter_name_or_path None
Initilized True layers
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules={'down_proj', 'up_proj', 'k_proj', 'q_proj', 'v_proj'}, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: requires_grad=True
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: requires_grad=True
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: requires_grad=True
Loading dataset with split: train[:100000]
Running tokenizer on train dataset (num_proc=32):   0%|                                                                             | 0/100000 [00:00<?, ? examples/s]Running tokenizer on train dataset (num_proc=32):   0%|                                                                    | 60/100000 [00:00<19:48, 84.08 examples/s]Running tokenizer on train dataset (num_proc=32):   0%|▏                                                                 | 360/100000 [00:00<02:55, 568.55 examples/s]Running tokenizer on train dataset (num_proc=32):   1%|▌                                                                | 900/100000 [00:00<01:08, 1437.25 examples/s]Running tokenizer on train dataset (num_proc=32):   2%|█                                                               | 1740/100000 [00:01<00:35, 2777.53 examples/s]Running tokenizer on train dataset (num_proc=32):   4%|██▎                                                             | 3690/100000 [00:01<00:15, 6220.23 examples/s]Running tokenizer on train dataset (num_proc=32):   5%|███                                                             | 4830/100000 [00:01<00:12, 7426.69 examples/s]Running tokenizer on train dataset (num_proc=32):   6%|███▋                                                            | 5820/100000 [00:01<00:11, 7865.72 examples/s]Running tokenizer on train dataset (num_proc=32):   7%|████▋                                                           | 7230/100000 [00:01<00:11, 8174.45 examples/s]Running tokenizer on train dataset (num_proc=32):   8%|█████▍                                                          | 8490/100000 [00:01<00:09, 9240.98 examples/s]Running tokenizer on train dataset (num_proc=32):  10%|██████                                                          | 9570/100000 [00:01<00:14, 6220.35 examples/s]Running tokenizer on train dataset (num_proc=32):  10%|██████▌                                                        | 10440/100000 [00:02<00:16, 5415.73 examples/s]Running tokenizer on train dataset (num_proc=32):  14%|████████▋                                                     | 13950/100000 [00:02<00:07, 10781.95 examples/s]Running tokenizer on train dataset (num_proc=32):  16%|█████████▋                                                    | 15630/100000 [00:02<00:07, 10877.57 examples/s]Running tokenizer on train dataset (num_proc=32):  17%|██████████▌                                                   | 17070/100000 [00:02<00:07, 10742.43 examples/s]Running tokenizer on train dataset (num_proc=32):  18%|███████████▌                                                   | 18390/100000 [00:02<00:08, 9589.25 examples/s]Running tokenizer on train dataset (num_proc=32):  20%|████████████▎                                                  | 19530/100000 [00:02<00:08, 9535.66 examples/s]Running tokenizer on train dataset (num_proc=32):  21%|████████████▉                                                  | 20610/100000 [00:03<00:09, 8688.66 examples/s]Running tokenizer on train dataset (num_proc=32):  22%|█████████████▌                                                 | 21570/100000 [00:03<00:08, 8830.07 examples/s]Running tokenizer on train dataset (num_proc=32):  23%|██████████████▏                                                | 22530/100000 [00:03<00:08, 8782.09 examples/s]Running tokenizer on train dataset (num_proc=32):  23%|██████████████▊                                                | 23460/100000 [00:03<00:08, 8771.16 examples/s]Running tokenizer on train dataset (num_proc=32):  24%|███████████████▎                                               | 24390/100000 [00:03<00:08, 8721.81 examples/s]Running tokenizer on train dataset (num_proc=32):  25%|███████████████▉                                               | 25290/100000 [00:03<00:08, 8704.97 examples/s]Running tokenizer on train dataset (num_proc=32):  26%|████████████████▍                                              | 26190/100000 [00:03<00:08, 8676.25 examples/s]Running tokenizer on train dataset (num_proc=32):  27%|█████████████████                                              | 27090/100000 [00:03<00:08, 8542.34 examples/s]Running tokenizer on train dataset (num_proc=32):  28%|█████████████████▋                                             | 27990/100000 [00:03<00:08, 8661.75 examples/s]Running tokenizer on train dataset (num_proc=32):  29%|██████████████████▏                                            | 28890/100000 [00:04<00:09, 7484.86 examples/s]Running tokenizer on train dataset (num_proc=32):  30%|██████████████████▊                                            | 29940/100000 [00:04<00:08, 8256.82 examples/s]Running tokenizer on train dataset (num_proc=32):  31%|███████████████████▍                                           | 30810/100000 [00:04<00:08, 8281.41 examples/s]Running tokenizer on train dataset (num_proc=32):  32%|███████████████████▉                                           | 31680/100000 [00:04<00:08, 8330.63 examples/s]Running tokenizer on train dataset (num_proc=32):  33%|████████████████████▌                                          | 32550/100000 [00:04<00:08, 8170.93 examples/s]Running tokenizer on train dataset (num_proc=32):  33%|█████████████████████                                          | 33420/100000 [00:04<00:08, 8273.81 examples/s]Running tokenizer on train dataset (num_proc=32):  34%|█████████████████████▋                                         | 34380/100000 [00:04<00:07, 8632.86 examples/s]Running tokenizer on train dataset (num_proc=32):  35%|██████████████████████▏                                        | 35280/100000 [00:04<00:07, 8667.61 examples/s]Running tokenizer on train dataset (num_proc=32):  36%|██████████████████████▊                                        | 36180/100000 [00:04<00:07, 8605.76 examples/s]Running tokenizer on train dataset (num_proc=32):  37%|███████████████████████▎                                       | 37050/100000 [00:04<00:07, 8478.13 examples/s]Running tokenizer on train dataset (num_proc=32):  38%|███████████████████████▉                                       | 37950/100000 [00:05<00:08, 7411.72 examples/s]Running tokenizer on train dataset (num_proc=32):  39%|████████████████████████▍                                      | 38820/100000 [00:05<00:07, 7746.40 examples/s]Running tokenizer on train dataset (num_proc=32):  40%|█████████████████████████                                      | 39840/100000 [00:05<00:07, 8409.06 examples/s]Running tokenizer on train dataset (num_proc=32):  41%|█████████████████████████▋                                     | 40800/100000 [00:05<00:06, 8471.85 examples/s]Running tokenizer on train dataset (num_proc=32):  42%|██████████████████████████▎                                    | 41700/100000 [00:05<00:06, 8508.71 examples/s]Running tokenizer on train dataset (num_proc=32):  43%|██████████████████████████▊                                    | 42570/100000 [00:05<00:06, 8507.08 examples/s]Running tokenizer on train dataset (num_proc=32):  43%|███████████████████████████▎                                   | 43440/100000 [00:05<00:06, 8370.12 examples/s]Running tokenizer on train dataset (num_proc=32):  44%|███████████████████████████▉                                   | 44310/100000 [00:05<00:06, 8313.14 examples/s]Running tokenizer on train dataset (num_proc=32):  45%|████████████████████████████▌                                  | 45300/100000 [00:05<00:06, 8760.93 examples/s]Running tokenizer on train dataset (num_proc=32):  46%|█████████████████████████████                                  | 46200/100000 [00:06<00:06, 8702.79 examples/s]Running tokenizer on train dataset (num_proc=32):  47%|█████████████████████████████▋                                 | 47100/100000 [00:06<00:06, 8719.71 examples/s]Running tokenizer on train dataset (num_proc=32):  48%|██████████████████████████████▏                                | 48000/100000 [00:06<00:05, 8728.22 examples/s]Running tokenizer on train dataset (num_proc=32):  49%|██████████████████████████████▊                                | 48900/100000 [00:06<00:05, 8683.48 examples/s]Running tokenizer on train dataset (num_proc=32):  50%|███████████████████████████████▎                               | 49770/100000 [00:06<00:06, 7564.72 examples/s]Running tokenizer on train dataset (num_proc=32):  51%|███████████████████████████████▉                               | 50670/100000 [00:06<00:06, 7775.25 examples/s]Running tokenizer on train dataset (num_proc=32):  52%|████████████████████████████████▌                              | 51600/100000 [00:06<00:05, 8152.22 examples/s]Running tokenizer on train dataset (num_proc=32):  52%|█████████████████████████████████                              | 52440/100000 [00:06<00:05, 8143.37 examples/s]Running tokenizer on train dataset (num_proc=32):  53%|█████████████████████████████████▌                             | 53280/100000 [00:06<00:05, 8018.18 examples/s]Running tokenizer on train dataset (num_proc=32):  54%|██████████████████████████████████                             | 54120/100000 [00:07<00:05, 8109.99 examples/s]Running tokenizer on train dataset (num_proc=32):  55%|██████████████████████████████████▌                            | 54960/100000 [00:07<00:05, 7972.93 examples/s]Running tokenizer on train dataset (num_proc=32):  56%|███████████████████████████████████▏                           | 55770/100000 [00:07<00:05, 7957.56 examples/s]Running tokenizer on train dataset (num_proc=32):  57%|███████████████████████████████████▋                           | 56610/100000 [00:07<00:05, 8078.40 examples/s]Running tokenizer on train dataset (num_proc=32):  57%|████████████████████████████████████▏                          | 57450/100000 [00:07<00:05, 8146.26 examples/s]Running tokenizer on train dataset (num_proc=32):  58%|████████████████████████████████████▋                          | 58290/100000 [00:07<00:05, 8132.57 examples/s]Running tokenizer on train dataset (num_proc=32):  59%|█████████████████████████████████████▎                         | 59250/100000 [00:07<00:04, 8553.63 examples/s]Running tokenizer on train dataset (num_proc=32):  60%|█████████████████████████████████████▉                         | 60120/100000 [00:07<00:04, 8458.23 examples/s]Running tokenizer on train dataset (num_proc=32):  61%|██████████████████████████████████████▍                        | 60990/100000 [00:07<00:05, 7367.29 examples/s]Running tokenizer on train dataset (num_proc=32):  62%|██████████████████████████████████████▉                        | 61800/100000 [00:08<00:05, 7554.32 examples/s]Running tokenizer on train dataset (num_proc=32):  63%|███████████████████████████████████████▍                       | 62670/100000 [00:08<00:04, 7857.77 examples/s]Running tokenizer on train dataset (num_proc=32):  64%|████████████████████████████████████████                       | 63510/100000 [00:08<00:04, 8008.17 examples/s]Running tokenizer on train dataset (num_proc=32):  64%|████████████████████████████████████████▌                      | 64350/100000 [00:08<00:04, 8028.18 examples/s]Running tokenizer on train dataset (num_proc=32):  65%|█████████████████████████████████████████                      | 65250/100000 [00:08<00:04, 8190.01 examples/s]Running tokenizer on train dataset (num_proc=32):  66%|█████████████████████████████████████████▋                     | 66090/100000 [00:08<00:04, 8029.57 examples/s]Running tokenizer on train dataset (num_proc=32):  67%|██████████████████████████████████████████▏                    | 67020/100000 [00:08<00:03, 8257.81 examples/s]Running tokenizer on train dataset (num_proc=32):  68%|██████████████████████████████████████████▊                    | 67860/100000 [00:08<00:03, 8175.88 examples/s]Running tokenizer on train dataset (num_proc=32):  69%|███████████████████████████████████████████▎                   | 68700/100000 [00:08<00:03, 8226.01 examples/s]Running tokenizer on train dataset (num_proc=32):  70%|███████████████████████████████████████████▊                   | 69540/100000 [00:08<00:03, 8201.49 examples/s]Running tokenizer on train dataset (num_proc=32):  70%|████████████████████████████████████████████▍                  | 70470/100000 [00:09<00:03, 8438.24 examples/s]Running tokenizer on train dataset (num_proc=32):  71%|████████████████████████████████████████████▉                  | 71400/100000 [00:09<00:03, 8594.26 examples/s]Running tokenizer on train dataset (num_proc=32):  72%|█████████████████████████████████████████████▌                 | 72270/100000 [00:09<00:03, 8474.45 examples/s]Running tokenizer on train dataset (num_proc=32):  73%|██████████████████████████████████████████████▏                | 73230/100000 [00:09<00:03, 7629.27 examples/s]Running tokenizer on train dataset (num_proc=32):  74%|██████████████████████████████████████████████▋                | 74015/100000 [00:09<00:03, 7654.50 examples/s]Running tokenizer on train dataset (num_proc=32):  75%|███████████████████████████████████████████████▎               | 75005/100000 [00:09<00:03, 8211.95 examples/s]Running tokenizer on train dataset (num_proc=32):  76%|███████████████████████████████████████████████▊               | 75845/100000 [00:09<00:02, 8083.97 examples/s]Running tokenizer on train dataset (num_proc=32):  77%|████████████████████████████████████████████████▎              | 76775/100000 [00:09<00:02, 8416.35 examples/s]Running tokenizer on train dataset (num_proc=32):  78%|████████████████████████████████████████████████▉              | 77645/100000 [00:09<00:02, 8265.28 examples/s]Running tokenizer on train dataset (num_proc=32):  78%|█████████████████████████████████████████████████▍             | 78485/100000 [00:10<00:02, 8186.20 examples/s]Running tokenizer on train dataset (num_proc=32):  79%|█████████████████████████████████████████████████▉             | 79325/100000 [00:10<00:02, 8214.01 examples/s]Running tokenizer on train dataset (num_proc=32):  80%|██████████████████████████████████████████████████▌            | 80165/100000 [00:10<00:02, 8125.28 examples/s]Running tokenizer on train dataset (num_proc=32):  81%|███████████████████████████████████████████████████            | 81095/100000 [00:10<00:02, 8442.24 examples/s]Running tokenizer on train dataset (num_proc=32):  82%|███████████████████████████████████████████████████▋           | 81970/100000 [00:10<00:02, 7136.15 examples/s]Running tokenizer on train dataset (num_proc=32):  83%|████████████████████████████████████████████████████▏          | 82900/100000 [00:10<00:02, 7673.51 examples/s]Running tokenizer on train dataset (num_proc=32):  84%|████████████████████████████████████████████████████▋          | 83710/100000 [00:10<00:02, 7722.86 examples/s]Running tokenizer on train dataset (num_proc=32):  85%|█████████████████████████████████████████████████████▎         | 84700/100000 [00:10<00:01, 8264.45 examples/s]Running tokenizer on train dataset (num_proc=32):  86%|█████████████████████████████████████████████████████▉         | 85570/100000 [00:10<00:01, 8216.35 examples/s]Running tokenizer on train dataset (num_proc=32):  86%|██████████████████████████████████████████████████████▍        | 86420/100000 [00:11<00:01, 7842.33 examples/s]Running tokenizer on train dataset (num_proc=32):  87%|███████████████████████████████████████████████████████        | 87420/100000 [00:11<00:01, 8283.74 examples/s]Running tokenizer on train dataset (num_proc=32):  88%|███████████████████████████████████████████████████████▌       | 88265/100000 [00:11<00:01, 8193.19 examples/s]Running tokenizer on train dataset (num_proc=32):  89%|████████████████████████████████████████████████████████▎      | 89415/100000 [00:11<00:01, 9120.90 examples/s]Running tokenizer on train dataset (num_proc=32):  90%|████████████████████████████████████████████████████████▉      | 90445/100000 [00:11<00:01, 9300.45 examples/s]Running tokenizer on train dataset (num_proc=32):  92%|█████████████████████████████████████████████████████████▋     | 91585/100000 [00:11<00:00, 9568.78 examples/s]Running tokenizer on train dataset (num_proc=32):  93%|█████████████████████████████████████████████████████████▌    | 92765/100000 [00:11<00:00, 10158.35 examples/s]Running tokenizer on train dataset (num_proc=32):  94%|██████████████████████████████████████████████████████████▏   | 93850/100000 [00:11<00:00, 10323.88 examples/s]Running tokenizer on train dataset (num_proc=32):  95%|███████████████████████████████████████████████████████████   | 95330/100000 [00:11<00:00, 11588.80 examples/s]Running tokenizer on train dataset (num_proc=32):  97%|███████████████████████████████████████████████████████████▉  | 96610/100000 [00:12<00:00, 11903.37 examples/s]Running tokenizer on train dataset (num_proc=32):  98%|████████████████████████████████████████████████████████████▋ | 97820/100000 [00:12<00:00, 11811.81 examples/s]Running tokenizer on train dataset (num_proc=32):  99%|█████████████████████████████████████████████████████████████▍| 99040/100000 [00:12<00:00, 11331.56 examples/s]Running tokenizer on train dataset (num_proc=32): 100%|██████████████████████████████████████████████████████████████| 100000/100000 [00:13<00:00, 7658.93 examples/s]
  0%|                                                                                                                                       | 0/18750 [00:00<?, ?it/s]  0%|                                                                                                                            | 1/18750 [00:03<18:56:11,  3.64s/it]                                                                                                                                                                      {'loss': 0.8366, 'grad_norm': 0.6140286922454834, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
  0%|                                                                                                                            | 1/18750 [00:03<18:56:11,  3.64s/it]  0%|                                                                                                                            | 2/18750 [00:06<17:21:14,  3.33s/it]                                                                                                                                                                      {'loss': 0.8422, 'grad_norm': 0.5815786123275757, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
  0%|                                                                                                                            | 2/18750 [00:06<17:21:14,  3.33s/it]  0%|                                                                                                                            | 3/18750 [00:09<16:46:59,  3.22s/it]                                                                                                                                                                      {'loss': 0.8254, 'grad_norm': 0.5885604619979858, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.0}
  0%|                                                                                                                            | 3/18750 [00:09<16:46:59,  3.22s/it]  0%|                                                                                                                            | 4/18750 [00:12<16:30:26,  3.17s/it]                                                                                                                                                                      {'loss': 0.8412, 'grad_norm': 0.6495023369789124, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.0}
  0%|                                                                                                                            | 4/18750 [00:12<16:30:26,  3.17s/it]  0%|                                                                                                                            | 5/18750 [00:15<16:12:51,  3.11s/it]                                                                                                                                                                      {'loss': 0.7707, 'grad_norm': 0.516982913017273, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.0}
  0%|                                                                                                                            | 5/18750 [00:15<16:12:51,  3.11s/it]  0%|                                                                                                                            | 6/18750 [00:18<16:04:44,  3.09s/it]                                                                                                                                                                      {'loss': 0.8526, 'grad_norm': 0.6662994027137756, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.0}
  0%|                                                                                                                            | 6/18750 [00:18<16:04:44,  3.09s/it]  0%|                                                                                                                            | 7/18750 [00:22<15:57:25,  3.06s/it]                                                                                                                                                                      {'loss': 0.7843, 'grad_norm': 0.591097354888916, 'learning_rate': 2.1e-05, 'epoch': 0.0}
  0%|                                                                                                                            | 7/18750 [00:22<15:57:25,  3.06s/it]  0%|                                                                                                                            | 8/18750 [00:25<16:03:46,  3.09s/it]                                                                                                                                                                      {'loss': 0.6607, 'grad_norm': 0.4912300705909729, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.0}
  0%|                                                                                                                            | 8/18750 [00:25<16:03:46,  3.09s/it]  0%|                                                                                                                            | 9/18750 [00:28<15:58:40,  3.07s/it]                                                                                                                                                                      {'loss': 0.7558, 'grad_norm': 0.5412936210632324, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.0}
  0%|                                                                                                                            | 9/18750 [00:28<15:58:40,  3.07s/it]  0%|                                                                                                                           | 10/18750 [00:31<15:53:31,  3.05s/it]                                                                                                                                                                      {'loss': 0.8502, 'grad_norm': 0.7193308472633362, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.0}
  0%|                                                                                                                           | 10/18750 [00:31<15:53:31,  3.05s/it]  0%|                                                                                                                           | 11/18750 [00:34<15:55:43,  3.06s/it]                                                                                                                                                                      {'loss': 0.671, 'grad_norm': 0.6683577299118042, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.0}
  0%|                                                                                                                           | 11/18750 [00:34<15:55:43,  3.06s/it][rank0]: Traceback (most recent call last):
[rank0]:   File "/root/autodl-tmp/MiLoRA/train.py", line 299, in <module>
[rank0]:     train()
[rank0]:   File "/root/autodl-tmp/MiLoRA/train.py", line 292, in train
[rank0]:     trainer.train()
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/transformers/trainer.py", line 1859, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/transformers/trainer.py", line 2203, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/transformers/trainer.py", line 3147, in training_step
[rank0]:     self.accelerator.backward(loss)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/accelerate/accelerator.py", line 2116, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1976, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2056, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/root/miniconda3/envs/milora/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 
  0%|                                                                                                                           | 11/18750 [00:36<17:16:42,  3.32s/it]
[2024-10-07 11:37:08,930] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 5064
[2024-10-07 11:37:08,931] [ERROR] [launch.py:325:sigkill_handler] ['/root/miniconda3/envs/milora/bin/python', '-u', 'train.py', '--local_rank=0', '--deepspeed', 'configs/stage2.conf', '--model_name_or_path', '/root/autodl-tmp/meta-llama/Llama-2-7b-hf', '--output_dir', '/math/LLM-Adapters-lora-LR-3e-4--EPOCHS-3-rank-64', '--lora_r', '64', '--lora_alpha', '128', '--lora_dropout', '0.05', '--target_modules', 'q_proj,k_proj,v_proj,up_proj,down_proj', '--data_path', 'meta-math/MetaMathQA', '--dataset_split', 'train[:100000]', '--dataset_field', 'query', 'response', '--model_max_length', '2048', '--num_train_epochs', '3', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--save_strategy', 'epoch', '--learning_rate', '3e-4', '--weight_decay', '0.', '--warmup_ratio', '0', '--warmup_steps', '100', '--lr_scheduler_type', 'linear', '--logging_steps', '1', '--bf16', 'True', '--tf32', 'True', '--method_type', 'lora', '--report_to', 'tensorboard'] exits with return code = 1
